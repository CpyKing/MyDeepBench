"ID","Process ID","Process Name","Host Name","Kernel Name","Context","Stream","Block Size","Grid Size","Device","CC","Section Name","Metric Name","Metric Unit","Metric Value","Rule Name","Rule Type","Rule Description"
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","871.75",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.23",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","1463061",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","35.09",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.05",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","msecond","1.19",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","52.89",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","35.09",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","822281.72",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","5.02",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Block Size","","64",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Grid Size","","64",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","48",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","byte","0",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Threads","thread","4096",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.04",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit Registers","block","20",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","32",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit Warps","block","32",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","40",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","62.50",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Achieved Occupancy","%","3.00",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","1.92",
"0","16848","gemm_bench","127.0.0.1","generate_seed_pseudo(unsigned long long, unsigned long long, unsigned long long, curandOrdering, curandStateXORWOW *, unsigned int *)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (62.5%) is limited by the number of required registers. The difference between calculated theoretical (62.5%) and measured achieved occupancy (3.0%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","855.61",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.19",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","41378",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","46.19",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","46.19",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","34.66",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","34.04",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","22.63",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","31294.94",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","14.46",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Block Size","","64",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Grid Size","","64",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","32",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","byte","0",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Threads","thread","4096",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.03",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit Registers","block","32",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","32",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit Warps","block","32",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","64",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","100",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Achieved Occupancy","%","3.12",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","2.00",
"1","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated theoretical (100.0%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","822.43",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.14",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","40436",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","76.14",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","76.14",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","35.23",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","24.40",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","32.20",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","38071.40",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","15.27",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","Memory is more heavily utilized than Compute: Look at the Memory Workload Analysis section to identify the DRAM bottleneck. Check memory replay (coalescing) metrics to make sure you're efficiently utilizing the bytes transferred. Also consider whether it is possible to do more work per memory access (kernel fusion) or whether there are values you can (re)compute."
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Launch Statistics","Block Size","","256",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Launch Statistics","Grid Size","","8192",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","16",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","byte","0",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Launch Statistics","Threads","thread","2097152",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Launch Statistics","Waves Per SM","","12.80",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Occupancy","Block Limit Registers","block","16",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","32",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Occupancy","Block Limit Warps","block","8",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","64",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","100",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Occupancy","Achieved Occupancy","%","87.32",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","55.89",
"2","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(8192, 1, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated theoretical (100.0%) and measured achieved occupancy (87.3%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","751.27",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.05",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","6632",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","18.10",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","4.06",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","6.30",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","26.39",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","18.10",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","3650.05",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","5.96",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.0 full waves across all SMs. Look at Launch Statistics for more details."
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Block Size","","64",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Grid Size","","64",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","32",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","byte","0",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Threads","thread","4096",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.03",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 64 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit Registers","block","32",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","32",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Block Limit Warps","block","32",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","64",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","100",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Achieved Occupancy","%","3.10",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","1.98",
"3","16848","gemm_bench","127.0.0.1","void gen_sequenced<curandStateXORWOW, float, int, &curand_uniform_noargs<curandStateXORWOW>>(T1 *, T2 *, unsigned long, unsigned long, T3)","1","7","(64, 1, 1)","(64, 1, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated theoretical (100.0%) and measured achieved occupancy (3.1%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","729.73",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.02",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","4865",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","29.63",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","29.63",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","4.74",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","14.80",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","16.83",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","2768.49",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.16",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.8 full waves across all SMs. Look at Launch Statistics for more details."
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Block Size","","256",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Grid Size","","512",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","16",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","byte","0",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Threads","thread","131072",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.80",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Block Limit Registers","block","16",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","32",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Block Limit Warps","block","8",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","64",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","100",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Achieved Occupancy","%","57.89",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","37.05",
"4","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>, thrust::cuda_cub::__transform::unary_transform_f<thrust::device_ptr<float>, thrust::device_ptr<unsigned short>, thrust::cuda_cub::__transform::no_stencil_tag, thrust::identity<float>, thrust::cuda_cub::__transform::always_true_predicate>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated theoretical (100.0%) and measured achieved occupancy (57.9%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","762.14",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.06",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","3509",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","8.74",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","0.01",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","3.30",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","20.83",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","8.74",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","1170.50",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","8.89",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.8 full waves across all SMs. Look at Launch Statistics for more details."
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Block Size","","256",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Grid Size","","512",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","16",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","byte","0",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","byte/block","0",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Threads","thread","131072",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.80",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Block Limit Registers","block","16",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","32",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Block Limit Warps","block","8",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","64",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","100",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Achieved Occupancy","%","35.57",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","22.77",
"5","16848","gemm_bench","127.0.0.1","void thrust::cuda_cub::core::_kernel_agent<thrust::cuda_cub::__parallel_for::ParallelForAgent<thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>, thrust::cuda_cub::__fill::functor<thrust::device_ptr<unsigned short>, float>, long>(T2, T3)","1","7","(256, 1, 1)","(512, 1, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy is not impacted by any block limit. The difference between calculated theoretical (100.0%) and measured achieved occupancy (35.6%) can be the result of warp scheduling overheads or workload imbalances during the kernel execution. Load imbalances can occur between warps within a block as well as across blocks of the same kernel. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","829.79",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.17",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107105",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.11",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.11",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","91.74",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","55.06",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.67",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20820.11",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.31",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"6","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","825.60",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107290",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.08",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.08",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.38",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","55.15",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.65",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20786.74",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.29",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"7","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","835.55",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.17",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107597",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.05",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.05",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","91.55",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.66",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.62",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20969.71",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.25",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"8","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","829.11",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107395",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.07",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.07",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.13",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.71",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.64",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20952.16",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.27",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"9","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","826.56",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","106968",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.11",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.11",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.06",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.93",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.68",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20870.44",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.32",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"10","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","837.90",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.18",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107253",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.09",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.09",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","91.01",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.98",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.65",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20847.66",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.29",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"11","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","821.92",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.15",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107007",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.13",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.13",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.54",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.86",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.68",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20896.70",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.32",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"12","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","874.40",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.23",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","115209",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","11.26",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","11.26",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","93.63",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","50.75",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.41",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","22580.08",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","11.45",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"13","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","825.10",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107395",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.07",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.07",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.58",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.93",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.64",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20868.25",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.27",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"14","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","829.02",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107382",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.07",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.07",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.13",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","55.02",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.64",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20834.14",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.28",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"15","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","824.57",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107144",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.09",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.09",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.48",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.85",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.67",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20901.03",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.30",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"16","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","823.33",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107368",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.07",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.07",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.74",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.91",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.64",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20873.92",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.28",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"17","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","828.63",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","106955",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.12",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.12",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","91.78",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","55.08",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.69",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20811.08",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.33",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"18","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","827.31",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107462",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.06",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.06",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.35",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.88",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.64",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20890.09",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.27",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"19","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","818.15",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.15",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","106674",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.15",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.15",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.74",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.90",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.71",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20879.56",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.35",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"20","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","822.24",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.15",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107341",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.08",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.08",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.80",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.83",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.65",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20906.55",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.28",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"21","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","828.36",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107539",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.06",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.06",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.29",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.85",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.63",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20898.94",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.26",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"22","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","818.65",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.15",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107056",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.11",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.11",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.99",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.80",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.68",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20916.95",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.31",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"23","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","823.33",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","106693",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.14",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.14",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.19",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","55.00",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.71",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20842.34",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.35",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"24","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","826.35",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107423",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.07",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.07",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.42",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.88",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.64",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20888.86",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.27",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"25","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","831.07",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.17",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107287",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.08",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.08",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","91.78",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","55.02",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.65",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20834.62",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.29",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"26","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","826.95",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","106956",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.13",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.13",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","91.90",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.88",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.69",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20888.16",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.33",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"27","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","823.80",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","106989",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.11",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.11",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.35",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.87",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.68",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20889.94",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.32",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"28","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","828.81",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107150",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.09",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.09",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","91.97",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.86",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.67",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20894.58",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.31",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"29","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","824.84",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","106976",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.11",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.11",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.26",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.83",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.68",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20904.96",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.32",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"30","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","825.64",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107061",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.10",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.10",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.22",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.95",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.67",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20859.51",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.31",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"31","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","829.62",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107264",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.08",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.08",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","91.94",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.90",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.65",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20881.30",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.29",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"32","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","826.24",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107599",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.05",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.05",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.54",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.86",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.63",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20894.38",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.26",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"33","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","820.41",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.15",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107102",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.10",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.10",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.83",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.94",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.67",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20864.70",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.30",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"34","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","825.24",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","106475",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.17",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.17",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","91.81",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.82",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.73",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20910.53",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.38",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"35","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Frequency","cycle/usecond","828.41",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Frequency","cycle/nsecond","1.16",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Elapsed Cycles","cycle","107192",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Memory Throughput","%","12.09",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","DRAM Throughput","%","12.09",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Duration","usecond","92.03",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L1/TEX Cache Throughput","%","54.97",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","L2 Cache Throughput","%","10.66",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","SM Active Cycles","cycle","20852.97",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","GPU Speed Of Light Throughput","Compute (SM) Throughput","%","12.30",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","SpeedOfLight","","","","SOLBottleneck","WRN","This kernel grid is too small to fill the available resources on this device, resulting in only 0.1 full waves across all SMs. Look at Launch Statistics for more details."
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Block Size","","128",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Function Cache Configuration","","CachePreferNone",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Grid Size","","16",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Registers Per Thread","register/thread","154",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Shared Memory Configuration Size","Kbyte","98.30",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Driver Shared Memory Per Block","byte/block","0",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Dynamic Shared Memory Per Block","byte/block","0",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Static Shared Memory Per Block","Kbyte/block","41.22",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Threads","thread","2048",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Launch Statistics","Waves Per SM","","0.10",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","LaunchStats","","","","LaunchConfiguration","WRN","The grid for this launch is configured to execute only 16 blocks, which is less than the GPU's 80 multiprocessors. This can underutilize some multiprocessors. If you do not intend to execute this kernel concurrently with other workloads, consider reducing the block size to have at least one block per multiprocessor or increase the size of the grid to fully utilize the available hardware resources. See the Hardware Model (https://docs.nvidia.com/nsight-compute/ProfilingGuide/index.html#metrics-hw-model) description for more details on launch configurations."
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit SM","block","32",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Registers","block","3",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Shared Mem","block","2",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Block Limit Warps","block","16",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Active Warps per SM","warp","8",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Theoretical Occupancy","%","12.50",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Occupancy","%","6.25",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","Achieved Active Warps Per SM","warp","4.00",
"36","16848","gemm_bench","127.0.0.1","volta_h884gemm_256x64_ldg8_nn","1","7","(128, 1, 1)","(8, 2, 1)","0","7.0","Occupancy","","","","Occupancy","WRN","This kernel's theoretical occupancy (12.5%) is limited by the required amount of shared memory. See the CUDA Best Practices Guide (https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#occupancy) for more details on optimizing occupancy."
